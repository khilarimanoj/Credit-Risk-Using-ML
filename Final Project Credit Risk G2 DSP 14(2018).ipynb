{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'D:/Document/Python Scripts/XYZCorp_LendingData.txt',delimiter='\\t',parse_dates=[\"issue_d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data.dtypes)\n",
    "print(data.describe())\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data file into df create new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create half count variable with 50 % data condiation \n",
    "Remove columns which missing value(na) >50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_count=len(df)/2\n",
    "df1=df.dropna(thresh=half_count,axis=1)# del variable (axis=1)with >50% values as na\n",
    "print(df1.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barplot releationship status b/w some actual variales vs default_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Purpose' vs 'Default_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.countplot(x='default_ind',data=df1, palette='hls')\n",
    "plt.show()\n",
    "pd.crosstab(df1.purpose,df1.default_ind).plot(kind='bar')\n",
    "plt.title('defult for purpose Title')\n",
    "plt.xlabel('purpose')\n",
    "plt.ylabel('default_ind')\n",
    "plt.savefig('purpose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Emp_length' vs 'Default_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.crosstab(df1.emp_length,df1.default_ind).plot(kind='bar')\n",
    "plt.title('defult for emp_length Title')\n",
    "plt.xlabel('emp_length')\n",
    "plt.ylabel('default_ind')\n",
    "plt.savefig('emp_length')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Grade' vs 'Default_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df1.grade,df1.default_ind).plot(kind='bar')\n",
    "plt.title('defult for grade Title')\n",
    "plt.xlabel('grade')\n",
    "plt.ylabel('default_ind')\n",
    "plt.savefig('grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine our dataframes correlation matrix and drop highly correlated/redundant data to address multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cor = df1.corr()\n",
    "cor.loc[:,:] = np.tril(cor, k=-1)  # below main lower triangle of an array\n",
    "cor = cor.stack()\n",
    "print(cor)\n",
    "cor[(cor > 0.60) | (cor < -0.6)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigned Label of Categorical data and converting numerical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade Columns Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.grade.value_counts()\n",
    "grade_final={'A':6,'B':5,'C':4,'D':3,'E':2,'F':1,'G':0}\n",
    "df1.grade=[grade_final[item]for item in df1.grade]\n",
    "print(df1.grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Columns Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.term.value_counts()\n",
    "df1.term=df1.term.str.extract('(\\d+)')\n",
    "term_final={'36':0,'60':1}\n",
    "df1.term=[term_final[item]for item in df1.term]\n",
    "print(df1.term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emp_Length Columns Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['emp_length'].mode()[0]\n",
    "df1['emp_length'].fillna(df1['emp_length'].mode()[0],inplace=True)\n",
    "df1['emp_length'] = df1['emp_length'].str.replace('+','')\n",
    "df1['emp_length'] = df1['emp_length'].str.replace('<','')\n",
    "print(df1.emp_length)\n",
    "df1.emp_length=df1.emp_length.str.extract('(\\d+)')\n",
    "print(df1.emp_length)\n",
    "df1.emp_length = [int(x) for x in df1.emp_length]\n",
    "print(df1.emp_length.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Verification_statusColumns Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['verification_status'].unique())\n",
    "print(df1.verification_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ver_status={'Verified':1,'Source Verified':1,'Not Verified':0}\n",
    "df1.verification_status=[Ver_status[item]for item in df1.verification_status]\n",
    "print(df1.verification_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Ownership Columns Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['home_ownership'].unique())\n",
    "home_ownership_score={'NONE':0,'ANY':0,'OTHER':0,'RENT':1,'MORTGAGE':2,'OWN':3}\n",
    "df1.home_ownership=[home_ownership_score[item]for item in df1.home_ownership]\n",
    "print(df1.home_ownership)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application_type Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['application_type'].unique())\n",
    "App_type={'INDIVIDUAL':1,'JOINT':2}\n",
    "df1.application_type=[App_type[item] for item in df1.application_type]\n",
    "print(df1.application_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial_list_status Columns Labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['initial_list_status'].unique())\n",
    "list_status={'w':1,'f':2}\n",
    "df1.initial_list_status=[list_status[item] for item in df1.initial_list_status]\n",
    "print(df1.initial_list_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose Columns Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['purpose'].unique())\n",
    "purpose_score={'vacation':0,'other':1,'major_purchase':2,'car':3,'wedding':4,'medical':5,'moving':6,'home_improvement':7,'credit_card':8,'debt_consolidation':9,'house':10,'small_business':11,'renewable_energy':12,'educational':13}\n",
    "df1.purpose=[purpose_score[item] for item in df1.purpose]\n",
    "print(df1.purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Account Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.open_acc.mode()\n",
    "\n",
    "def open_acc_final(i):\n",
    "    if i in range (4,23):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "\n",
    "df1['open_acc']= df1['open_acc'].apply(open_acc_final)\n",
    "print(df1.open_acc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.revol_util.describe()\n",
    "print(df1.revol_util.isnull().sum())\n",
    "df1['revol_util'].fillna(df1['revol_util'].median(),inplace=True)\n",
    "print(df1.revol_util.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earliest_cr_line date columns Labealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['earliest_cr_line'].nunique()\n",
    "df1.earliest_cr_line=df1.earliest_cr_line.str.extract('(\\d+)')\n",
    "print(df1.earliest_cr_line)\n",
    "df1.earliest_cr_line = [int(x) for x in df1.earliest_cr_line]\n",
    "print(df1.earliest_cr_line.dtype)\n",
    "#%%\n",
    "def earliest_cr_line_final(i):\n",
    "    if i in range(44,79):\n",
    "        return(0)\n",
    "    elif i in range(80,90):\n",
    "        return(1)\n",
    "    elif i in range(90,0):\n",
    "        return(2)\n",
    "    else:\n",
    "        return(3)\n",
    "#%%\n",
    "df1['earliest_cr_line']= df1['earliest_cr_line'].apply(earliest_cr_line_final)\n",
    "print(df1.earliest_cr_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last_pymnt_d date columns Labealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['last_pymnt_d'].fillna(df1['last_pymnt_d'].mode()[0],inplace=True)\n",
    "df1.last_pymnt_d=df1.last_pymnt_d.str.extract('(\\d+)')\n",
    "print(df1.last_pymnt_d)\n",
    "df1.last_pymnt_d = [int(x) for x in df1.last_pymnt_d]\n",
    "print(df1.last_pymnt_d.dtype)\n",
    "#%%\n",
    "def last_pymnt_d_final(i):\n",
    "    if i in range (8,10):\n",
    "        return(0)\n",
    "    elif i in range(11,13):\n",
    "        return(1)    \n",
    "    else:\n",
    "        return(2)\n",
    "    #%%\n",
    "df1['last_pymnt_d']= df1['last_pymnt_d'].apply(last_pymnt_d_final)\n",
    "print(df1.last_pymnt_d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Less Important Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(df1)\n",
    "df2.drop(['member_id','funded_amnt','addr_state','emp_title','sub_grade','inq_last_6mths','funded_amnt_inv','pymnt_plan','title','zip_code','inq_last_6mths','total_acc','out_prncp_inv','total_pymnt_inv','next_pymnt_d','policy_code','tot_coll_amt'],axis=1,inplace=True)\n",
    "\n",
    "#%%\n",
    "df2.shape\n",
    "df2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop na obervation present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.tot_cur_bal.isnull().sum())\n",
    "print(df2.total_rev_hi_lim.isnull().sum())\n",
    "df3=df2.dropna(axis=0,how='any',thresh=None)\n",
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check heatmap multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df3.corr()\n",
    "sns.heatmap(corrmat, vmax=1, square=True,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k,'revol_bal')['revol_bal'].index\n",
    "cm = np.corrcoef(df3[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last_credit_pull_d date columns Labealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['last_credit_pull_d'].unique())\n",
    "print(df3.last_credit_pull_d.isnull().sum())\n",
    "df3.last_credit_pull_d=df3.last_credit_pull_d.str.extract('(\\d+)')\n",
    "print(df3.last_credit_pull_d)\n",
    "\n",
    "df3.last_credit_pull_d = [int(x) for x in df3.last_credit_pull_d]\n",
    "print(df3.last_credit_pull_d.dtype)\n",
    "\n",
    "def last__credit_pull_d_final(i):\n",
    "    if i in range (14,16):\n",
    "        return(0)\n",
    "    else:\n",
    "        return(1)\n",
    "        \n",
    "df3['last_credit_pull_d']= df3['last_credit_pull_d'].apply(last__credit_pull_d_final)\n",
    "print(df3.last_credit_pull_d) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pub_Rec Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['pub_rec'].unique())\n",
    "df3.pub_rec.isnull().sum()\n",
    "#%%\n",
    "def pub_rec_final(i):\n",
    "    if i == 0:\n",
    "        return(4)\n",
    "    elif i == 1:\n",
    "        return(3)\n",
    "    elif i == 2:\n",
    "        return(2)\n",
    "    elif i == 3:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "df3['pub_rec']= df3['pub_rec'].apply(pub_rec_final)\n",
    "print(df3.pub_rec) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delinq_2yrs Labealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.delinq_2yrs.isnull().sum()\n",
    "#%%\n",
    "def delinq_2yrs_final(i):\n",
    "    if i == 0:\n",
    "        return(4)\n",
    "    elif i == 1:\n",
    "        return(3)\n",
    "    elif i == 2:\n",
    "        return(2)\n",
    "    elif i == 3:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "df3['delinq_2yrs']= df3['delinq_2yrs'].apply(delinq_2yrs_final)\n",
    "print(df3.delinq_2yrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test data with issue_d date.( Train < 2015-06-01,test>= 2015-06-01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3.issue_d\n",
    "train_data=df3[df3['issue_d']<'2015-06-01']\n",
    "test_data=df3[df3['issue_d']>='2015-06-01']\n",
    "train_data.shape\n",
    "test_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop issue_d train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data=train_data.drop('issue_d',axis=1)\n",
    "test_data=test_data.drop('issue_d',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train\n",
    "X_train=train_data.values[:,1:-1]\n",
    "Y_train=train_data.values[:,-1]\n",
    "#%% Test\n",
    "X_test=test_data.values[:,1:-1]\n",
    "Y_test=test_data.values[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Logistic Regression Model and Predicting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = (LogisticRegression())\n",
    "LR.fit(X_train, Y_train)\n",
    "Y_pred=LR.predict(X_test)\n",
    "print(list(zip(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model using confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cfm=confusion_matrix(Y_test,Y_pred)\n",
    "print(cfm)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "acc=accuracy_score(Y_test,Y_pred)\n",
    "print(\"Accuracy of the model:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking prediction probability manual changing threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_prob=LR.predict_proba(X_test)\n",
    "print(y_pred_prob)\n",
    "\n",
    "\n",
    "y_pred_class=[]\n",
    "for value in y_pred_prob[:,0]:\n",
    "    if value < 0.70:\n",
    "        y_pred_class.append(1)\n",
    "    else:\n",
    "        y_pred_class.append(0)\n",
    "        \n",
    "        \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cfm=confusion_matrix(Y_test.tolist(),y_pred_class)\n",
    "print(cfm)\n",
    "acc=accuracy_score(Y_test.tolist(),y_pred_class)\n",
    "\n",
    "print(\"Accuracy of the model: \", acc)\n",
    "print(classification_report(Y_test.tolist(),y_pred_class)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running for loop for treshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for a in np.arange(0,1,0.05):\n",
    "\n",
    "    predict_mine=np.where(y_pred_prob[:,0] < a, 1 ,0)\n",
    "cfm=confusion_matrix(Y_test.tolist(),predict_mine)\n",
    "total_err=cfm[0,1]+cfm[1,0] \n",
    "print(\"erros at threshold \",a,\":\",total_err,\",type 2 error :\", cfm[1,0],\"type 1 error :\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC checking,plot Receiver operating Characteristic(ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "fpr, tpr,threshold = metrics.roc_curve(Y_test.tolist(),y_pred_class)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print(auc)\n",
    "\n",
    "\n",
    "plt.title('Receiver operating Characteristic')\n",
    "plt.plot(fpr, tpr,'b',label = auc) \n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model 2 = predicting using the extratreesclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model=(ExtraTreesClassifier(101))\n",
    "#fit the model on data and predict values\n",
    "model=model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "#confusion matrix\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC checking\n",
    "from sklearn import metrics\n",
    "fpr, tpr,threshold = metrics.roc_curve(Y_test,Y_pred)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print(auc)\n",
    "# plot Receiver operating Characteristic(ROC) model 2 extra tree classifier\n",
    "\n",
    "plt.title('Receiver operating Characteristic')\n",
    "plt.plot(fpr, tpr,'b',label = auc) \n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
